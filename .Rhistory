pca_scores <- data.frame(pca_data$x[,1:2])
pca_scores$diagnosis <- clean_ovarian_dataset$diagnosis
# load the ggplot library
library(ggplot2)
ggplot(pca_scores, aes(x = PC1, y = PC2, color = diagnosis)) +
geom_point(alpha = 0.7, size = 2) +
labs(title = "PCA of Ovarian Dataset (First 2 Principal Components)",
x = "Principal Component 1",
y = "Principal Component 2") +
theme_minimal()
# Note that we used chatGPT for formatting and coloring.
# The prompt was: "what do you think of my last plot? is it correct? + code"
# The code that was submitted was the code from the tutorial:
# install.packages("devtools")
# library(devtools)
# install.packages("remotes")
# remotes::install_github("vqv/ggbiplot")
# library(ggbiplot)
# ggbiplot(mtcars.pca)
area_concavity_plot <- ggplot(clean_ovarian_dataset, aes(x = area, y = concavity, color = diagnosis)) +
geom_point(alpha = 0.8, size = 2) +
labs(title = "Area vs concavity",
x = "Area",
y = "Concavity") +
theme_minimal()
print(area_concavity_plot)
boxplot(pca_data$x,
main = "Distribution of Principal Components",
xlab = "Principal Components",
ylab = "Transformed Values",
outline = FALSE)
# Scale the data
clean_scaled_ovarian_dataset <- scale(clean_ovarian_dataset[c(3:ncol(clean_ovarian_dataset))], center = TRUE, scale = TRUE)
# K_mean clustering with 2 centers
k_mean_clean_ovarian_data <- kmeans(clean_scaled_ovarian_dataset, centers = 2)
# Extract the prediction vector from the clustering type
predictions_vector <- k_mean_clean_ovarian_data$cluster
# Create a vector of true labels
true_labels_vector <- clean_ovarian_dataset$diagnosis
# Case 1
# Convert the the cluster labels to diagnosis labels
predictions_vector_1 <- ifelse(predictions_vector == 1, "M", "B")
# Compare the model predictions to true labels
confusion_matrix_1 <- table(predictions_vector_1, true_labels_vector)
# Compute accuracy
accuracy_1 <- sum(diag(confusion_matrix_1)/sum(confusion_matrix_1))
# Case 2
# Convert the the cluster labels to diagnosis labels
predictions_vector_2 <- ifelse(predictions_vector == 2, "M", "B")
# Compare the model predictions to true labels
confusion_matrix_2 <- table(predictions_vector_2, true_labels_vector)
# Compute accuracy
accuracy_2 <- sum(diag(confusion_matrix_2)/sum(confusion_matrix_2))
# Report the confusion matrix, accuracy, precision, and recall of the model with the best accuracy
if (accuracy_1 > accuracy_2){
print(confusion_matrix_1)
cat("\nAccuracy:", accuracy_1,
"\nPrecision:", confusion_matrix_1[1,1] / (confusion_matrix_1[1,1] + confusion_matrix_1[1,2]),
"\nRecall:", confusion_matrix_1[1,1] / (confusion_matrix_1[1,1] + confusion_matrix_1[2,1]))
} else {
print(confusion_matrix_2)
cat("\nAccuracy:", accuracy_2,
"\nPrecision:", confusion_matrix_2[1,1] / (confusion_matrix_2[1,1] + confusion_matrix_2[1,2]),
"\nRecall:", confusion_matrix_2[1,1] / (confusion_matrix_2[1,1] + confusion_matrix_2[2,1]))
}
total_accuracy <- 0
for (i in 1:10){
# K_mean clustering with 2 centers
k_mean_clean_ovarian_data <- kmeans(clean_scaled_ovarian_dataset, centers = 2)
# Extract the prediction vector from the clustering type
predictions_vector <- k_mean_clean_ovarian_data$cluster
# Create a vector of true labels
true_labels_vector <- clean_ovarian_dataset$diagnosis
# Case 1
# Convert the the cluster labels to diagnosis labels
predictions_vector_1 <- ifelse(predictions_vector == 1, "M", "B")
# Compare the model predictions to true labels
confusion_matrix_1 <- table(predictions_vector_1, true_labels_vector)
# Compute accuracy
accuracy_1 <- sum(diag(confusion_matrix_1)/sum(confusion_matrix_1))
# Case 2
# Convert the the cluster labels to diagnosis labels
predictions_vector_2 <- ifelse(predictions_vector == 2, "M", "B")
# Compare the model predictions to true labels
confusion_matrix_2 <- table(predictions_vector_2, true_labels_vector)
# Compute accuracy
accuracy_2 <- sum(diag(confusion_matrix_2)/sum(confusion_matrix_2))
# Add the accuracy of the model to the total
if (accuracy_1 > accuracy_2){
total_accuracy <- total_accuracy + accuracy_1
} else {
total_accuracy <- total_accuracy + accuracy_2
}
}
Average_accuracy <- total_accuracy/10
cat("The average accuracy across 10 k_means model is: ", Average_accuracy)
total_accuracy <- 0
for (i in 1:10){
# K_mean clustering with 2 centers
k_mean_clean_pca_data <- kmeans(pca_data$x[,c(1:5)], centers = 2)
# Extract the prediction vector from the clustering type
predictions_vector <- k_mean_clean_pca_data$cluster
# Case 1
# Convert the the cluster labels to diagnosis labels
predictions_vector_1 <- ifelse(predictions_vector == 1, "M", "B")
# Compare the model predictions to true labels
confusion_matrix_1 <- table(predictions_vector_1, true_labels_vector)
# Compute accuracy
accuracy_1 <- sum(diag(confusion_matrix_1)/sum(confusion_matrix_1))
# Case 2
# Convert the the cluster labels to diagnosis labels
predictions_vector_2 <- ifelse(predictions_vector == 2, "M", "B")
# Compare the model predictions to true labels
confusion_matrix_2 <- table(predictions_vector_2, true_labels_vector)
# Compute accuracy
accuracy_2 <- sum(diag(confusion_matrix_2)/sum(confusion_matrix_2))
# Add the accuracy of the model to the total
if (accuracy_1 > accuracy_2){
total_accuracy <- total_accuracy + accuracy_1
} else {
total_accuracy <- total_accuracy + accuracy_2
}
}
Average_accuracy <- total_accuracy/10
cat("The average accuracy across 10 k_means model is: ", Average_accuracy)
ovarian.dataset.train <- ovarian.dataset[sample(nrow(ovarian.dataset))[1:(nrow(ovarian.dataset)/2)],]
ovarian.dataset.test <- ovarian.dataset[sample(nrow(ovarian.dataset))[(nrow(ovarian.dataset)/2):(nrow(ovarian.dataset))],]
# Load the ISLR library
library(ISLR)
# Data exploration
#install.packages("corrplot")
# Load the corrplot library
library(corrplot)
# scale the data
scaled_ovarian_dataset_train <- scale(ovarian.dataset.train[c(3:ncol(ovarian.dataset.train))], center = TRUE, scale = TRUE)
scaled_ovarian_dataset_test <- scale(ovarian.dataset.test[c(3:ncol(ovarian.dataset.train))], center = TRUE, scale = TRUE)
# Look for correlation between variables
correlations <- cor(scaled_ovarian_dataset_train[,1:30])
corrplot(correlations, method="circle")
# Prepare the data for fitting and testing
scaled_ovarian_dataset_train_and_diagnosis <- data.frame(
diagnosis = ovarian.dataset.train[, 2],
scaled_ovarian_dataset_train
)
scaled_ovarian_dataset_test_and_diagnosis <- data.frame(
diagnosis = ovarian.dataset.test[, 2],
scaled_ovarian_dataset_test
)
colnames(scaled_ovarian_dataset_train_and_diagnosis)[1] <- "diagnosis"
colnames(scaled_ovarian_dataset_test_and_diagnosis)[1] <- "diagnosis"
scaled_ovarian_dataset_train_and_diagnosis$diagnosis <- factor(
scaled_ovarian_dataset_train_and_diagnosis$diagnosis,
levels = c("B", "M")
)
scaled_ovarian_dataset_test_and_diagnosis$diagnosis <- factor(
scaled_ovarian_dataset_test_and_diagnosis$diagnosis,
levels = c("B", "M")
)
# Fit a logistic regression on the training set
glm.fit <- glm(diagnosis ~ perimeter + area + smoothness + symmetry + concavity + protein1 + protein2 + protein3 + protein4 + protein5 + protein6 + protein7 + protein8 + protein9 + protein10 + protein11 + protein12 + protein13 + protein14 + protein15 + protein16 + protein17 + protein18 + protein19 + protein20 + protein21 + protein22 + protein23 + protein24 + protein25, data = scaled_ovarian_dataset_train_and_diagnosis, family = binomial)
# summary(glm.fit)
# Report the accuracy on the train set
glm.probs.train <- predict(glm.fit, scaled_ovarian_dataset_train_and_diagnosis, type = "response")
glm.pred.train <- ifelse(glm.probs.train > 0.5, "M", "B")
glm.pred.train <- factor(glm.pred.train, levels = c("B", "M"))
train_conf_matrix <- table(glm.pred.train, scaled_ovarian_dataset_train_and_diagnosis$diagnosis)
print(train_conf_matrix)
cat("\nAccuracy:", sum(diag(train_conf_matrix)/sum(train_conf_matrix)),
"\nPrecision:", train_conf_matrix[1,1] / (train_conf_matrix[1,1] + train_conf_matrix[1,2]),
"\nRecall:", train_conf_matrix[1,1] / (train_conf_matrix[1,1] + train_conf_matrix[2,1]),
"\n")
# Report the accuracy on the test set
glm.probs.test <- predict(glm.fit, scaled_ovarian_dataset_test_and_diagnosis, type = "response")
glm.pred.test <- ifelse(glm.probs.test > 0.5, "M", "B")
glm.pred.test <- factor(glm.pred.test, levels = c("B", "M"))
test_conf_matrix <- table(glm.pred.test, scaled_ovarian_dataset_test_and_diagnosis$diagnosis)
print(test_conf_matrix)
cat("\nAccuracy:", sum(diag(test_conf_matrix))/sum(test_conf_matrix),
"\nPrecision:", test_conf_matrix[1,1] / (test_conf_matrix[1,1] + test_conf_matrix[1,2]),
"\nRecall:", test_conf_matrix[1,1] / (test_conf_matrix[1,1] + test_conf_matrix[2,1]))
train_pca <- prcomp(scaled_ovarian_dataset_train, center = TRUE, scale. = TRUE)
#top 5 only
train_pcs <- data.frame(train_pca$x[, 1:5])
test_pcs <- data.frame(predict(train_pca, newdata = scaled_ovarian_dataset_test)[, 1:5])
#combine pcs with diagnosis label
train_pcs$diagnosis <- ovarian.dataset.train$diagnosis
test_pcs$diagnosis <- ovarian.dataset.test$diagnosis
# Make the first column a factor and not a list of characters
train_pcs$diagnosis <- factor(
train_pcs$diagnosis,
levels = c("B", "M")
)
test_pcs$diagnosis <- factor(
test_pcs$diagnosis,
levels = c("B", "M")
)
#fit model
glm.fit <- glm(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5, data = train_pcs, family = binomial)
#training performace
glm.probs.train.pca <- predict(glm.fit, train_pcs, type = "response")
glm.pred.train.pca <- ifelse(glm.probs.train.pca > 0.5, "M", "B")
glm.pred.train.pca <- factor(glm.pred.train.pca, levels = c("B", "M"))
train_conf_matrix_pca <- table(glm.pred.train.pca, train_pcs$diagnosis)
print(train_conf_matrix_pca)
cat("\nTraining Set Performance:\n")
cat("Accuracy:", sum(diag(train_conf_matrix_pca)) / sum(train_conf_matrix_pca),
"\nPrecision:", train_conf_matrix_pca[1,1] / (train_conf_matrix_pca[1,1] + train_conf_matrix_pca[1,2]),
"\nRecall:", train_conf_matrix_pca[1,1] / (train_conf_matrix_pca[1,1] + train_conf_matrix_pca[2,1]), "\n")
#test performance
glm.probs.test.pca <- predict(glm.fit, test_pcs, type = "response")
glm.pred.test.pca <- ifelse(glm.probs.test.pca > 0.5, "M", "B")
glm.pred.test.pca <- factor(glm.pred.test.pca, levels = c("B", "M"))
test_conf_matrix_pca <- table(glm.pred.test.pca, test_pcs$diagnosis)
print(test_conf_matrix_pca)
cat("\nTest Set Performance:\n")
cat("Accuracy:", sum(diag(test_conf_matrix_pca)) / sum(test_conf_matrix_pca),
"\nPrecision:", test_conf_matrix_pca[1,1] / (test_conf_matrix_pca[1,1] + test_conf_matrix_pca[1,2]),
"\nRecall:", test_conf_matrix_pca[1,1] / (test_conf_matrix_pca[1,1] + test_conf_matrix_pca[2,1]), "\n")
#install.packages("ROCR")
library(ROCR)
#predicted probabilities from trained model
pred.prob <- predict(glm.fit, test_pcs, type = "response")
#prediction object for ROCR
pred <- prediction(pred.prob, test_pcs$diagnosis, label.ordering = c("B", "M"))
#performance - true positive rate vs false positive rate)
perform <- performance(pred, "tpr", "fpr")
#plot ROC curve
plot(perform, colorize = TRUE, main = "ROC Curve for Logistic Regression Model")
abline(a = 0, b = 1, lty = 2, col = "grey")
#area under curve
auc <- performance(pred, measure = "auc")
auc_obj <- performance(pred, "auc")
auc_value <- unlist(slot(auc_obj, "y.values"))
cat("\nArea Under the Curve:", auc_value, "\n")
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
#fit decision tree
tree_model <- rpart(diagnosis ~ ., data = scaled_ovarian_dataset_train_and_diagnosis, method = "class")
#visualize
rpart.plot(tree_model, main = "Decision Tree for Diagnosis Classification")
#predictions
tree_pred_train <- predict(tree_model, scaled_ovarian_dataset_train_and_diagnosis, type = "class")
tree_pred_test <- predict(tree_model, scaled_ovarian_dataset_test_and_diagnosis, type = "class")
#train confusion matrix
train_conf_tree <- table(tree_pred_train, scaled_ovarian_dataset_train_and_diagnosis$diagnosis)
cat("\nTraining Performance:\n")
print(train_conf_tree)
cat("Accuracy:", sum(diag(train_conf_tree))/sum(train_conf_tree), "\n")
#test confusion matrix
test_conf_tree <- table(tree_pred_test, scaled_ovarian_dataset_test_and_diagnosis$diagnosis)
cat("\nTest Performance (Decision Tree):\n")
print(test_conf_tree)
cat("Accuracy:", sum(diag(test_conf_tree))/sum(test_conf_tree), "\n")
setwd("C:/Users/maxim/Desktop/BMEG 310/BMEG-310---Assignment-3")
#BiocManager::install("TCGAbiolinks")
BiocManager::install("TCGAbiolinks")
data_clinical_patient <- read.delim("/Users/maxim/Desktop/BMEG 310/Laboratory 7/data_clinical_patient.txt", header = TRUE, sep = "\t", skip = 4)
data_clinical_patient <- read.delim("/Users/maxim/Desktop/BMEG 310/Laboratory 7/data_clinical_patient.txt", header = TRUE, sep = "\t", skip = 4)
summary(data_clinical_patient)
data_clinical_patient <- read.delim("/Users/maxim/Desktop/BMEG 310/Laboratory 7/data_clinical_patient.txt", header = TRUE, sep = "\t", skip = 4)
head(data_clinical_patient)
BiocManager::install("survival")
BiocManager::install("survminer")
library("TCGAbiolinks")
library("survival")
library("survminer")
library(ggplot2)
library(ggplot2)
library("TCGAbiolinks")
library("survival")
library("survminer")
library("SummarizedExperiment")
tcga_data = readRDS(file = "tcga_data.RDS")
tcga_data = readRDS(file = "tcga_data.RDS")
colnames(colData(tcga_data))
tcga_data = readRDS(file = "tcga_data.RDS")
colnames(colData(tcga_data))
table(tcga_data@colData$bmi)
tcga_data = readRDS(file = "tcga_data.RDS")
clinical_data <- tcga_data@colData
library(ggplot2)
library("TCGAbiolinks")
library("survival")
library("survminer")
library("SummarizedExperiment")
tcga_data = readRDS(file = "tcga_data.RDS")
clinical_data <- tcga_data@colData
clinical_data$bmi.class <- ifelse(
is.na(clinical_data$bmi), NA,
ifelse(clinical_data$bmi < 18.5, "underweight",
ifelse(clinical_data$bmi >= 18.5 & clinical_data$bmi < 25, "healthy",
ifelse(clinical_data$bmi >= 25 & clinical_data$bmi < 30, "overweight",
"obese"
)
)
)
)
View(clinical_data)
clin_df = clinical[clinical$definition == "Primary solid Tumor",
c("patient",
"vital_status",
"days_to_death",
"bmi.class")]
clin_df = clinical_data[clinical_data$definition == "Primary solid Tumor",
c("patient",
"vital_status",
"days_to_death",
"bmi.class")]
View(clinical_data)
View(clin_df)
clin_df$deceased = clin_df$vital_status == "Dead"
clin_df = clinical_data[clinical_data$definition == "Primary solid Tumor",
c("patient",
"vital_status",
"days_to_last_follow_up",
"days_to_death",
"bmi.class")]
clin_df$deceased = clin_df$vital_status == "Dead"
# And to days_to_last_follow_up for patients who are still alive
clin_df$overall_survival = ifelse(clin_df$deceased,
clin_df$days_to_death,
clin_df$days_to_last_follow_up)
Surv(clin_df$overall_survival, clin_df$deceased)
Surv(clin_df$overall_survival, clin_df$deceased)
fit = survfit(Surv(overall_survival, deceased) ~ bmi.class, data=clin_df)
ggsurvplot(fit, data=clin_df)
ggsurvplot(fit, data=clin_df, pval=T, risk.table=T, risk.table.height=0.35)
ggsurvplot(fit, data=clin_df, pval=T, risk.table=T, risk.table.col="strata")
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T, risk.table=T, risk.table.col="strata")
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
linetype = "strata"
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
risk.table.col = "strata",
tables.theme = theme_cleantable(),
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
tables.theme = theme_cleantable(),
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
tables.theme = theme_cleantable(),
risk.table.height=0.50
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
tables.theme = theme_cleantable(),
risk.table.height=0.35
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
tables.theme = theme_cleantable(),
risk.table.height=0.25
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
risk.table.height=0.25
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
risk.table.height=0.45
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
risk.table.height=0.35
)
ggsurvplot(
fit,
data=clin_df,
title = "Overall Survival by BMI Category in Primary Solid Tumors",
xlab = "Time (Days)",
ylab = "Overall Survival Probability",
legend.title = "BMI Class",
legend.labs = c("Healthy", "Obese", "Overweight", "Underweight"),
font.main = c(14, "bold"),
font.x = c(12),
font.y = c(12),
pval=T,
risk.table=T,
risk.table.col="strata",
risk.table.height=0.35,
censor = FALSE
)
library(ggplot2)
library("TCGAbiolinks")
library("survival")
library("survminer")
library("SummarizedExperiment")
